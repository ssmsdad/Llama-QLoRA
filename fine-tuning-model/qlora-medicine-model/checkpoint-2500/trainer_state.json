{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.3265359434439746,
  "eval_steps": 500,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013061437737758985,
      "grad_norm": 16.01875877380371,
      "learning_rate": 1.9988246049366597e-05,
      "loss": 12.3872,
      "step": 10
    },
    {
      "epoch": 0.002612287547551797,
      "grad_norm": 12.3085298538208,
      "learning_rate": 1.9975186104218364e-05,
      "loss": 9.6141,
      "step": 20
    },
    {
      "epoch": 0.003918431321327695,
      "grad_norm": 8.104395866394043,
      "learning_rate": 1.9962126159070136e-05,
      "loss": 8.0773,
      "step": 30
    },
    {
      "epoch": 0.005224575095103594,
      "grad_norm": 7.102978229522705,
      "learning_rate": 1.9949066213921903e-05,
      "loss": 7.5613,
      "step": 40
    },
    {
      "epoch": 0.006530718868879492,
      "grad_norm": 5.128921031951904,
      "learning_rate": 1.9936006268773674e-05,
      "loss": 7.2884,
      "step": 50
    },
    {
      "epoch": 0.00783686264265539,
      "grad_norm": 4.0293474197387695,
      "learning_rate": 1.9922946323625442e-05,
      "loss": 7.2462,
      "step": 60
    },
    {
      "epoch": 0.00914300641643129,
      "grad_norm": 3.9230613708496094,
      "learning_rate": 1.9909886378477213e-05,
      "loss": 7.2046,
      "step": 70
    },
    {
      "epoch": 0.010449150190207188,
      "grad_norm": 6.403261661529541,
      "learning_rate": 1.989682643332898e-05,
      "loss": 7.0761,
      "step": 80
    },
    {
      "epoch": 0.011755293963983086,
      "grad_norm": 3.829475164413452,
      "learning_rate": 1.9883766488180752e-05,
      "loss": 7.0598,
      "step": 90
    },
    {
      "epoch": 0.013061437737758985,
      "grad_norm": 3.2918193340301514,
      "learning_rate": 1.987070654303252e-05,
      "loss": 7.0611,
      "step": 100
    },
    {
      "epoch": 0.014367581511534883,
      "grad_norm": 3.617158889770508,
      "learning_rate": 1.985764659788429e-05,
      "loss": 7.0147,
      "step": 110
    },
    {
      "epoch": 0.01567372528531078,
      "grad_norm": 4.131974697113037,
      "learning_rate": 1.984458665273606e-05,
      "loss": 6.9933,
      "step": 120
    },
    {
      "epoch": 0.016979869059086678,
      "grad_norm": 4.318906307220459,
      "learning_rate": 1.983152670758783e-05,
      "loss": 6.9823,
      "step": 130
    },
    {
      "epoch": 0.01828601283286258,
      "grad_norm": 4.349148273468018,
      "learning_rate": 1.9818466762439598e-05,
      "loss": 6.9655,
      "step": 140
    },
    {
      "epoch": 0.019592156606638475,
      "grad_norm": 4.057341575622559,
      "learning_rate": 1.980540681729137e-05,
      "loss": 6.9408,
      "step": 150
    },
    {
      "epoch": 0.020898300380414375,
      "grad_norm": 3.670686721801758,
      "learning_rate": 1.9792346872143137e-05,
      "loss": 6.9313,
      "step": 160
    },
    {
      "epoch": 0.022204444154190272,
      "grad_norm": 4.039196491241455,
      "learning_rate": 1.9779286926994908e-05,
      "loss": 6.9327,
      "step": 170
    },
    {
      "epoch": 0.023510587927966172,
      "grad_norm": 3.923943042755127,
      "learning_rate": 1.9766226981846676e-05,
      "loss": 6.9286,
      "step": 180
    },
    {
      "epoch": 0.02481673170174207,
      "grad_norm": 4.063487529754639,
      "learning_rate": 1.975316703669845e-05,
      "loss": 6.9199,
      "step": 190
    },
    {
      "epoch": 0.02612287547551797,
      "grad_norm": 3.8965320587158203,
      "learning_rate": 1.9740107091550218e-05,
      "loss": 6.9239,
      "step": 200
    },
    {
      "epoch": 0.027429019249293866,
      "grad_norm": 4.630425930023193,
      "learning_rate": 1.972704714640199e-05,
      "loss": 6.8489,
      "step": 210
    },
    {
      "epoch": 0.028735163023069766,
      "grad_norm": 4.563963413238525,
      "learning_rate": 1.9713987201253757e-05,
      "loss": 6.75,
      "step": 220
    },
    {
      "epoch": 0.030041306796845663,
      "grad_norm": 4.079813003540039,
      "learning_rate": 1.9700927256105528e-05,
      "loss": 6.8571,
      "step": 230
    },
    {
      "epoch": 0.03134745057062156,
      "grad_norm": 3.975285291671753,
      "learning_rate": 1.9687867310957296e-05,
      "loss": 6.8573,
      "step": 240
    },
    {
      "epoch": 0.03265359434439746,
      "grad_norm": 3.8940789699554443,
      "learning_rate": 1.9674807365809067e-05,
      "loss": 6.8612,
      "step": 250
    },
    {
      "epoch": 0.033959738118173356,
      "grad_norm": 4.384311676025391,
      "learning_rate": 1.9661747420660835e-05,
      "loss": 6.8642,
      "step": 260
    },
    {
      "epoch": 0.035265881891949256,
      "grad_norm": 3.621760129928589,
      "learning_rate": 1.9648687475512606e-05,
      "loss": 6.8012,
      "step": 270
    },
    {
      "epoch": 0.03657202566572516,
      "grad_norm": 3.9796864986419678,
      "learning_rate": 1.9635627530364373e-05,
      "loss": 6.9182,
      "step": 280
    },
    {
      "epoch": 0.03787816943950105,
      "grad_norm": 4.312295436859131,
      "learning_rate": 1.9622567585216145e-05,
      "loss": 6.8181,
      "step": 290
    },
    {
      "epoch": 0.03918431321327695,
      "grad_norm": 3.7535269260406494,
      "learning_rate": 1.9609507640067912e-05,
      "loss": 6.8813,
      "step": 300
    },
    {
      "epoch": 0.04049045698705285,
      "grad_norm": 4.3181376457214355,
      "learning_rate": 1.9596447694919684e-05,
      "loss": 6.7393,
      "step": 310
    },
    {
      "epoch": 0.04179660076082875,
      "grad_norm": 3.9531781673431396,
      "learning_rate": 1.958338774977145e-05,
      "loss": 6.7503,
      "step": 320
    },
    {
      "epoch": 0.043102744534604644,
      "grad_norm": 4.133495807647705,
      "learning_rate": 1.9570327804623222e-05,
      "loss": 6.9375,
      "step": 330
    },
    {
      "epoch": 0.044408888308380544,
      "grad_norm": 3.9601266384124756,
      "learning_rate": 1.955726785947499e-05,
      "loss": 6.8269,
      "step": 340
    },
    {
      "epoch": 0.045715032082156444,
      "grad_norm": 4.318766117095947,
      "learning_rate": 1.954420791432676e-05,
      "loss": 6.8752,
      "step": 350
    },
    {
      "epoch": 0.047021175855932344,
      "grad_norm": 4.435824394226074,
      "learning_rate": 1.953114796917853e-05,
      "loss": 6.8527,
      "step": 360
    },
    {
      "epoch": 0.04832731962970824,
      "grad_norm": 4.48759126663208,
      "learning_rate": 1.95180880240303e-05,
      "loss": 6.8312,
      "step": 370
    },
    {
      "epoch": 0.04963346340348414,
      "grad_norm": 4.186826705932617,
      "learning_rate": 1.950502807888207e-05,
      "loss": 6.806,
      "step": 380
    },
    {
      "epoch": 0.05093960717726004,
      "grad_norm": 4.010262966156006,
      "learning_rate": 1.949196813373384e-05,
      "loss": 6.8233,
      "step": 390
    },
    {
      "epoch": 0.05224575095103594,
      "grad_norm": 4.5500640869140625,
      "learning_rate": 1.947890818858561e-05,
      "loss": 6.8107,
      "step": 400
    },
    {
      "epoch": 0.05355189472481183,
      "grad_norm": 4.262691974639893,
      "learning_rate": 1.9465848243437378e-05,
      "loss": 6.8584,
      "step": 410
    },
    {
      "epoch": 0.05485803849858773,
      "grad_norm": 4.035707950592041,
      "learning_rate": 1.945278829828915e-05,
      "loss": 6.9224,
      "step": 420
    },
    {
      "epoch": 0.05616418227236363,
      "grad_norm": 4.605960369110107,
      "learning_rate": 1.943972835314092e-05,
      "loss": 6.8225,
      "step": 430
    },
    {
      "epoch": 0.05747032604613953,
      "grad_norm": 5.615077495574951,
      "learning_rate": 1.9426668407992688e-05,
      "loss": 6.8552,
      "step": 440
    },
    {
      "epoch": 0.058776469819915425,
      "grad_norm": 4.470644950866699,
      "learning_rate": 1.941360846284446e-05,
      "loss": 6.8996,
      "step": 450
    },
    {
      "epoch": 0.060082613593691325,
      "grad_norm": 4.502252101898193,
      "learning_rate": 1.9400548517696227e-05,
      "loss": 6.8247,
      "step": 460
    },
    {
      "epoch": 0.061388757367467225,
      "grad_norm": 5.00893497467041,
      "learning_rate": 1.9387488572547998e-05,
      "loss": 6.8539,
      "step": 470
    },
    {
      "epoch": 0.06269490114124313,
      "grad_norm": 5.128209114074707,
      "learning_rate": 1.9374428627399766e-05,
      "loss": 6.8902,
      "step": 480
    },
    {
      "epoch": 0.06400104491501903,
      "grad_norm": 4.3166117668151855,
      "learning_rate": 1.9361368682251537e-05,
      "loss": 6.8472,
      "step": 490
    },
    {
      "epoch": 0.06530718868879493,
      "grad_norm": 3.8407952785491943,
      "learning_rate": 1.9348308737103305e-05,
      "loss": 6.8796,
      "step": 500
    },
    {
      "epoch": 0.06661333246257081,
      "grad_norm": 4.527550220489502,
      "learning_rate": 1.9335248791955076e-05,
      "loss": 6.7859,
      "step": 510
    },
    {
      "epoch": 0.06791947623634671,
      "grad_norm": 5.185912609100342,
      "learning_rate": 1.9322188846806844e-05,
      "loss": 6.9185,
      "step": 520
    },
    {
      "epoch": 0.06922562001012261,
      "grad_norm": 4.1824517250061035,
      "learning_rate": 1.9309128901658615e-05,
      "loss": 6.8185,
      "step": 530
    },
    {
      "epoch": 0.07053176378389851,
      "grad_norm": 4.524024963378906,
      "learning_rate": 1.9296068956510382e-05,
      "loss": 6.8476,
      "step": 540
    },
    {
      "epoch": 0.07183790755767441,
      "grad_norm": 6.1317877769470215,
      "learning_rate": 1.9283009011362154e-05,
      "loss": 6.8706,
      "step": 550
    },
    {
      "epoch": 0.07314405133145031,
      "grad_norm": 4.454689025878906,
      "learning_rate": 1.9269949066213925e-05,
      "loss": 6.8284,
      "step": 560
    },
    {
      "epoch": 0.07445019510522621,
      "grad_norm": 4.091543674468994,
      "learning_rate": 1.9256889121065693e-05,
      "loss": 6.8307,
      "step": 570
    },
    {
      "epoch": 0.0757563388790021,
      "grad_norm": 4.344172477722168,
      "learning_rate": 1.9243829175917464e-05,
      "loss": 6.8702,
      "step": 580
    },
    {
      "epoch": 0.077062482652778,
      "grad_norm": 4.168149471282959,
      "learning_rate": 1.923076923076923e-05,
      "loss": 6.8094,
      "step": 590
    },
    {
      "epoch": 0.0783686264265539,
      "grad_norm": 4.234846591949463,
      "learning_rate": 1.9217709285621003e-05,
      "loss": 6.7592,
      "step": 600
    },
    {
      "epoch": 0.0796747702003298,
      "grad_norm": 4.662473201751709,
      "learning_rate": 1.920464934047277e-05,
      "loss": 6.8249,
      "step": 610
    },
    {
      "epoch": 0.0809809139741057,
      "grad_norm": 4.090307712554932,
      "learning_rate": 1.919158939532454e-05,
      "loss": 6.8387,
      "step": 620
    },
    {
      "epoch": 0.0822870577478816,
      "grad_norm": 4.494222164154053,
      "learning_rate": 1.917852945017631e-05,
      "loss": 6.8223,
      "step": 630
    },
    {
      "epoch": 0.0835932015216575,
      "grad_norm": 4.105278491973877,
      "learning_rate": 1.916546950502808e-05,
      "loss": 6.8677,
      "step": 640
    },
    {
      "epoch": 0.0848993452954334,
      "grad_norm": 4.660621166229248,
      "learning_rate": 1.915240955987985e-05,
      "loss": 6.9,
      "step": 650
    },
    {
      "epoch": 0.08620548906920929,
      "grad_norm": 3.944589138031006,
      "learning_rate": 1.913934961473162e-05,
      "loss": 6.8285,
      "step": 660
    },
    {
      "epoch": 0.08751163284298519,
      "grad_norm": 5.1247100830078125,
      "learning_rate": 1.912628966958339e-05,
      "loss": 6.8798,
      "step": 670
    },
    {
      "epoch": 0.08881777661676109,
      "grad_norm": 4.014522075653076,
      "learning_rate": 1.9113229724435158e-05,
      "loss": 6.9187,
      "step": 680
    },
    {
      "epoch": 0.09012392039053699,
      "grad_norm": 4.791286945343018,
      "learning_rate": 1.910016977928693e-05,
      "loss": 6.7831,
      "step": 690
    },
    {
      "epoch": 0.09143006416431289,
      "grad_norm": 4.441593170166016,
      "learning_rate": 1.9087109834138697e-05,
      "loss": 6.8637,
      "step": 700
    },
    {
      "epoch": 0.09273620793808879,
      "grad_norm": 4.484072685241699,
      "learning_rate": 1.9074049888990468e-05,
      "loss": 6.7795,
      "step": 710
    },
    {
      "epoch": 0.09404235171186469,
      "grad_norm": 4.255557060241699,
      "learning_rate": 1.9060989943842236e-05,
      "loss": 6.8019,
      "step": 720
    },
    {
      "epoch": 0.09534849548564059,
      "grad_norm": 4.9115447998046875,
      "learning_rate": 1.9047929998694007e-05,
      "loss": 6.8422,
      "step": 730
    },
    {
      "epoch": 0.09665463925941647,
      "grad_norm": 4.543349742889404,
      "learning_rate": 1.9034870053545778e-05,
      "loss": 6.7369,
      "step": 740
    },
    {
      "epoch": 0.09796078303319238,
      "grad_norm": 4.81358003616333,
      "learning_rate": 1.9021810108397546e-05,
      "loss": 6.815,
      "step": 750
    },
    {
      "epoch": 0.09926692680696828,
      "grad_norm": 4.216808795928955,
      "learning_rate": 1.9008750163249317e-05,
      "loss": 6.8834,
      "step": 760
    },
    {
      "epoch": 0.10057307058074418,
      "grad_norm": 4.739414215087891,
      "learning_rate": 1.8995690218101085e-05,
      "loss": 6.8039,
      "step": 770
    },
    {
      "epoch": 0.10187921435452008,
      "grad_norm": 5.394394397735596,
      "learning_rate": 1.8982630272952856e-05,
      "loss": 6.8648,
      "step": 780
    },
    {
      "epoch": 0.10318535812829598,
      "grad_norm": 4.5455851554870605,
      "learning_rate": 1.8969570327804624e-05,
      "loss": 6.8433,
      "step": 790
    },
    {
      "epoch": 0.10449150190207188,
      "grad_norm": 3.8985700607299805,
      "learning_rate": 1.8956510382656395e-05,
      "loss": 6.76,
      "step": 800
    },
    {
      "epoch": 0.10579764567584776,
      "grad_norm": 4.628332138061523,
      "learning_rate": 1.8943450437508163e-05,
      "loss": 6.8568,
      "step": 810
    },
    {
      "epoch": 0.10710378944962366,
      "grad_norm": 4.675403118133545,
      "learning_rate": 1.8930390492359934e-05,
      "loss": 6.7941,
      "step": 820
    },
    {
      "epoch": 0.10840993322339956,
      "grad_norm": 4.316625595092773,
      "learning_rate": 1.89173305472117e-05,
      "loss": 6.7153,
      "step": 830
    },
    {
      "epoch": 0.10971607699717546,
      "grad_norm": 4.292690753936768,
      "learning_rate": 1.8904270602063473e-05,
      "loss": 6.7585,
      "step": 840
    },
    {
      "epoch": 0.11102222077095136,
      "grad_norm": 3.788208484649658,
      "learning_rate": 1.889121065691524e-05,
      "loss": 6.9141,
      "step": 850
    },
    {
      "epoch": 0.11232836454472726,
      "grad_norm": 3.9359512329101562,
      "learning_rate": 1.887815071176701e-05,
      "loss": 6.7981,
      "step": 860
    },
    {
      "epoch": 0.11363450831850316,
      "grad_norm": 4.861037731170654,
      "learning_rate": 1.886509076661878e-05,
      "loss": 6.7229,
      "step": 870
    },
    {
      "epoch": 0.11494065209227906,
      "grad_norm": 3.939058542251587,
      "learning_rate": 1.885203082147055e-05,
      "loss": 6.7439,
      "step": 880
    },
    {
      "epoch": 0.11624679586605495,
      "grad_norm": 4.293900966644287,
      "learning_rate": 1.883897087632232e-05,
      "loss": 6.7234,
      "step": 890
    },
    {
      "epoch": 0.11755293963983085,
      "grad_norm": 4.629969120025635,
      "learning_rate": 1.882591093117409e-05,
      "loss": 6.8013,
      "step": 900
    },
    {
      "epoch": 0.11885908341360675,
      "grad_norm": 4.322476863861084,
      "learning_rate": 1.881285098602586e-05,
      "loss": 6.8724,
      "step": 910
    },
    {
      "epoch": 0.12016522718738265,
      "grad_norm": 4.225693702697754,
      "learning_rate": 1.879979104087763e-05,
      "loss": 6.7787,
      "step": 920
    },
    {
      "epoch": 0.12147137096115855,
      "grad_norm": 4.501260757446289,
      "learning_rate": 1.87867310957294e-05,
      "loss": 6.8305,
      "step": 930
    },
    {
      "epoch": 0.12277751473493445,
      "grad_norm": 4.509845733642578,
      "learning_rate": 1.877367115058117e-05,
      "loss": 6.8241,
      "step": 940
    },
    {
      "epoch": 0.12408365850871035,
      "grad_norm": 4.140361785888672,
      "learning_rate": 1.876061120543294e-05,
      "loss": 6.8732,
      "step": 950
    },
    {
      "epoch": 0.12538980228248625,
      "grad_norm": 4.899746894836426,
      "learning_rate": 1.874755126028471e-05,
      "loss": 6.7722,
      "step": 960
    },
    {
      "epoch": 0.12669594605626214,
      "grad_norm": 4.465937614440918,
      "learning_rate": 1.8734491315136477e-05,
      "loss": 6.7546,
      "step": 970
    },
    {
      "epoch": 0.12800208983003805,
      "grad_norm": 4.949455738067627,
      "learning_rate": 1.872143136998825e-05,
      "loss": 6.8576,
      "step": 980
    },
    {
      "epoch": 0.12930823360381394,
      "grad_norm": 4.455745697021484,
      "learning_rate": 1.8708371424840016e-05,
      "loss": 6.7883,
      "step": 990
    },
    {
      "epoch": 0.13061437737758985,
      "grad_norm": 4.31752872467041,
      "learning_rate": 1.8695311479691787e-05,
      "loss": 6.7529,
      "step": 1000
    },
    {
      "epoch": 0.13192052115136574,
      "grad_norm": 4.54829216003418,
      "learning_rate": 1.8682251534543555e-05,
      "loss": 6.6881,
      "step": 1010
    },
    {
      "epoch": 0.13322666492514162,
      "grad_norm": 4.841947078704834,
      "learning_rate": 1.8669191589395326e-05,
      "loss": 6.7822,
      "step": 1020
    },
    {
      "epoch": 0.13453280869891754,
      "grad_norm": 4.184478759765625,
      "learning_rate": 1.8656131644247094e-05,
      "loss": 6.6757,
      "step": 1030
    },
    {
      "epoch": 0.13583895247269342,
      "grad_norm": 6.172862529754639,
      "learning_rate": 1.8643071699098865e-05,
      "loss": 6.7076,
      "step": 1040
    },
    {
      "epoch": 0.13714509624646934,
      "grad_norm": 4.61326265335083,
      "learning_rate": 1.8630011753950633e-05,
      "loss": 6.7786,
      "step": 1050
    },
    {
      "epoch": 0.13845124002024523,
      "grad_norm": 4.416805267333984,
      "learning_rate": 1.8616951808802404e-05,
      "loss": 6.8013,
      "step": 1060
    },
    {
      "epoch": 0.13975738379402114,
      "grad_norm": 4.325121879577637,
      "learning_rate": 1.8603891863654172e-05,
      "loss": 6.8062,
      "step": 1070
    },
    {
      "epoch": 0.14106352756779703,
      "grad_norm": 4.017244338989258,
      "learning_rate": 1.8590831918505943e-05,
      "loss": 6.7514,
      "step": 1080
    },
    {
      "epoch": 0.1423696713415729,
      "grad_norm": 5.181612014770508,
      "learning_rate": 1.8577771973357714e-05,
      "loss": 6.7849,
      "step": 1090
    },
    {
      "epoch": 0.14367581511534883,
      "grad_norm": 4.24873685836792,
      "learning_rate": 1.8564712028209485e-05,
      "loss": 6.8267,
      "step": 1100
    },
    {
      "epoch": 0.1449819588891247,
      "grad_norm": 4.633494853973389,
      "learning_rate": 1.8551652083061253e-05,
      "loss": 6.8159,
      "step": 1110
    },
    {
      "epoch": 0.14628810266290063,
      "grad_norm": 4.115248203277588,
      "learning_rate": 1.8538592137913024e-05,
      "loss": 6.7511,
      "step": 1120
    },
    {
      "epoch": 0.1475942464366765,
      "grad_norm": 3.9692940711975098,
      "learning_rate": 1.8525532192764792e-05,
      "loss": 6.7811,
      "step": 1130
    },
    {
      "epoch": 0.14890039021045243,
      "grad_norm": 4.72607946395874,
      "learning_rate": 1.8512472247616563e-05,
      "loss": 6.803,
      "step": 1140
    },
    {
      "epoch": 0.1502065339842283,
      "grad_norm": 4.792342662811279,
      "learning_rate": 1.849941230246833e-05,
      "loss": 6.7477,
      "step": 1150
    },
    {
      "epoch": 0.1515126777580042,
      "grad_norm": 4.627171039581299,
      "learning_rate": 1.8486352357320102e-05,
      "loss": 6.7918,
      "step": 1160
    },
    {
      "epoch": 0.1528188215317801,
      "grad_norm": 4.298267841339111,
      "learning_rate": 1.847329241217187e-05,
      "loss": 6.8004,
      "step": 1170
    },
    {
      "epoch": 0.154124965305556,
      "grad_norm": 4.1030497550964355,
      "learning_rate": 1.846023246702364e-05,
      "loss": 6.8433,
      "step": 1180
    },
    {
      "epoch": 0.15543110907933191,
      "grad_norm": 4.270774841308594,
      "learning_rate": 1.844717252187541e-05,
      "loss": 6.7946,
      "step": 1190
    },
    {
      "epoch": 0.1567372528531078,
      "grad_norm": 3.8508825302124023,
      "learning_rate": 1.843411257672718e-05,
      "loss": 6.805,
      "step": 1200
    },
    {
      "epoch": 0.15804339662688371,
      "grad_norm": 5.826086521148682,
      "learning_rate": 1.8421052631578947e-05,
      "loss": 6.8199,
      "step": 1210
    },
    {
      "epoch": 0.1593495404006596,
      "grad_norm": 4.22096586227417,
      "learning_rate": 1.840799268643072e-05,
      "loss": 6.795,
      "step": 1220
    },
    {
      "epoch": 0.16065568417443551,
      "grad_norm": 4.263975620269775,
      "learning_rate": 1.8394932741282486e-05,
      "loss": 6.8077,
      "step": 1230
    },
    {
      "epoch": 0.1619618279482114,
      "grad_norm": 4.246059417724609,
      "learning_rate": 1.8381872796134257e-05,
      "loss": 6.7792,
      "step": 1240
    },
    {
      "epoch": 0.1632679717219873,
      "grad_norm": 4.413963317871094,
      "learning_rate": 1.8368812850986025e-05,
      "loss": 6.873,
      "step": 1250
    },
    {
      "epoch": 0.1645741154957632,
      "grad_norm": 4.253534317016602,
      "learning_rate": 1.8355752905837796e-05,
      "loss": 6.7853,
      "step": 1260
    },
    {
      "epoch": 0.1658802592695391,
      "grad_norm": 4.49181604385376,
      "learning_rate": 1.8342692960689567e-05,
      "loss": 6.7698,
      "step": 1270
    },
    {
      "epoch": 0.167186403043315,
      "grad_norm": 4.314443111419678,
      "learning_rate": 1.832963301554134e-05,
      "loss": 6.8114,
      "step": 1280
    },
    {
      "epoch": 0.1684925468170909,
      "grad_norm": 3.884044647216797,
      "learning_rate": 1.8316573070393106e-05,
      "loss": 6.7594,
      "step": 1290
    },
    {
      "epoch": 0.1697986905908668,
      "grad_norm": 4.072667121887207,
      "learning_rate": 1.8303513125244877e-05,
      "loss": 6.7762,
      "step": 1300
    },
    {
      "epoch": 0.1711048343646427,
      "grad_norm": 3.9215855598449707,
      "learning_rate": 1.8290453180096645e-05,
      "loss": 6.7479,
      "step": 1310
    },
    {
      "epoch": 0.17241097813841857,
      "grad_norm": 5.0106587409973145,
      "learning_rate": 1.8277393234948416e-05,
      "loss": 6.8328,
      "step": 1320
    },
    {
      "epoch": 0.1737171219121945,
      "grad_norm": 4.605123996734619,
      "learning_rate": 1.8264333289800184e-05,
      "loss": 6.6971,
      "step": 1330
    },
    {
      "epoch": 0.17502326568597038,
      "grad_norm": 4.843538761138916,
      "learning_rate": 1.8251273344651955e-05,
      "loss": 6.7691,
      "step": 1340
    },
    {
      "epoch": 0.1763294094597463,
      "grad_norm": 4.684720993041992,
      "learning_rate": 1.8238213399503723e-05,
      "loss": 6.7879,
      "step": 1350
    },
    {
      "epoch": 0.17763555323352218,
      "grad_norm": 4.780303955078125,
      "learning_rate": 1.8225153454355494e-05,
      "loss": 6.7113,
      "step": 1360
    },
    {
      "epoch": 0.1789416970072981,
      "grad_norm": 4.051076889038086,
      "learning_rate": 1.8212093509207262e-05,
      "loss": 6.7903,
      "step": 1370
    },
    {
      "epoch": 0.18024784078107398,
      "grad_norm": 4.883952617645264,
      "learning_rate": 1.8199033564059033e-05,
      "loss": 6.8224,
      "step": 1380
    },
    {
      "epoch": 0.18155398455484986,
      "grad_norm": 4.434271335601807,
      "learning_rate": 1.81859736189108e-05,
      "loss": 6.8543,
      "step": 1390
    },
    {
      "epoch": 0.18286012832862578,
      "grad_norm": 5.290702819824219,
      "learning_rate": 1.8172913673762572e-05,
      "loss": 6.8135,
      "step": 1400
    },
    {
      "epoch": 0.18416627210240166,
      "grad_norm": 4.588248252868652,
      "learning_rate": 1.815985372861434e-05,
      "loss": 6.8094,
      "step": 1410
    },
    {
      "epoch": 0.18547241587617758,
      "grad_norm": 4.4803009033203125,
      "learning_rate": 1.814679378346611e-05,
      "loss": 6.8685,
      "step": 1420
    },
    {
      "epoch": 0.18677855964995346,
      "grad_norm": 4.304361343383789,
      "learning_rate": 1.813373383831788e-05,
      "loss": 6.7333,
      "step": 1430
    },
    {
      "epoch": 0.18808470342372938,
      "grad_norm": 4.172392845153809,
      "learning_rate": 1.812067389316965e-05,
      "loss": 6.7808,
      "step": 1440
    },
    {
      "epoch": 0.18939084719750526,
      "grad_norm": 5.7087483406066895,
      "learning_rate": 1.810761394802142e-05,
      "loss": 6.7736,
      "step": 1450
    },
    {
      "epoch": 0.19069699097128118,
      "grad_norm": 4.184716701507568,
      "learning_rate": 1.809455400287319e-05,
      "loss": 6.7718,
      "step": 1460
    },
    {
      "epoch": 0.19200313474505706,
      "grad_norm": 5.606542587280273,
      "learning_rate": 1.808149405772496e-05,
      "loss": 6.7977,
      "step": 1470
    },
    {
      "epoch": 0.19330927851883295,
      "grad_norm": 4.114499568939209,
      "learning_rate": 1.806843411257673e-05,
      "loss": 6.8261,
      "step": 1480
    },
    {
      "epoch": 0.19461542229260886,
      "grad_norm": 3.589775800704956,
      "learning_rate": 1.80553741674285e-05,
      "loss": 6.6928,
      "step": 1490
    },
    {
      "epoch": 0.19592156606638475,
      "grad_norm": 5.642498970031738,
      "learning_rate": 1.804231422228027e-05,
      "loss": 6.7857,
      "step": 1500
    },
    {
      "epoch": 0.19722770984016066,
      "grad_norm": 3.8505992889404297,
      "learning_rate": 1.8029254277132038e-05,
      "loss": 6.8311,
      "step": 1510
    },
    {
      "epoch": 0.19853385361393655,
      "grad_norm": 4.737154483795166,
      "learning_rate": 1.801619433198381e-05,
      "loss": 6.7882,
      "step": 1520
    },
    {
      "epoch": 0.19983999738771246,
      "grad_norm": 4.582612037658691,
      "learning_rate": 1.8003134386835576e-05,
      "loss": 6.7205,
      "step": 1530
    },
    {
      "epoch": 0.20114614116148835,
      "grad_norm": 4.33701229095459,
      "learning_rate": 1.7990074441687348e-05,
      "loss": 6.8133,
      "step": 1540
    },
    {
      "epoch": 0.20245228493526424,
      "grad_norm": 4.24643087387085,
      "learning_rate": 1.7977014496539115e-05,
      "loss": 6.8225,
      "step": 1550
    },
    {
      "epoch": 0.20375842870904015,
      "grad_norm": 4.2868242263793945,
      "learning_rate": 1.7963954551390887e-05,
      "loss": 6.7891,
      "step": 1560
    },
    {
      "epoch": 0.20506457248281604,
      "grad_norm": 5.584061622619629,
      "learning_rate": 1.7950894606242654e-05,
      "loss": 6.7718,
      "step": 1570
    },
    {
      "epoch": 0.20637071625659195,
      "grad_norm": 5.228790760040283,
      "learning_rate": 1.7937834661094425e-05,
      "loss": 6.7328,
      "step": 1580
    },
    {
      "epoch": 0.20767686003036784,
      "grad_norm": 4.446795463562012,
      "learning_rate": 1.7924774715946193e-05,
      "loss": 6.7498,
      "step": 1590
    },
    {
      "epoch": 0.20898300380414375,
      "grad_norm": 4.094690322875977,
      "learning_rate": 1.7911714770797964e-05,
      "loss": 6.7434,
      "step": 1600
    },
    {
      "epoch": 0.21028914757791964,
      "grad_norm": 4.395391464233398,
      "learning_rate": 1.7898654825649732e-05,
      "loss": 6.8112,
      "step": 1610
    },
    {
      "epoch": 0.21159529135169552,
      "grad_norm": 4.2505340576171875,
      "learning_rate": 1.7885594880501503e-05,
      "loss": 6.7429,
      "step": 1620
    },
    {
      "epoch": 0.21290143512547144,
      "grad_norm": 4.601895332336426,
      "learning_rate": 1.7872534935353274e-05,
      "loss": 6.7123,
      "step": 1630
    },
    {
      "epoch": 0.21420757889924733,
      "grad_norm": 4.716131210327148,
      "learning_rate": 1.7859474990205042e-05,
      "loss": 6.8258,
      "step": 1640
    },
    {
      "epoch": 0.21551372267302324,
      "grad_norm": 4.302542209625244,
      "learning_rate": 1.7846415045056813e-05,
      "loss": 6.7201,
      "step": 1650
    },
    {
      "epoch": 0.21681986644679913,
      "grad_norm": 4.344365119934082,
      "learning_rate": 1.783335509990858e-05,
      "loss": 6.6916,
      "step": 1660
    },
    {
      "epoch": 0.21812601022057504,
      "grad_norm": 4.76187801361084,
      "learning_rate": 1.7820295154760352e-05,
      "loss": 6.7345,
      "step": 1670
    },
    {
      "epoch": 0.21943215399435093,
      "grad_norm": 5.48715353012085,
      "learning_rate": 1.780723520961212e-05,
      "loss": 6.7949,
      "step": 1680
    },
    {
      "epoch": 0.22073829776812684,
      "grad_norm": 4.853883743286133,
      "learning_rate": 1.779417526446389e-05,
      "loss": 6.8469,
      "step": 1690
    },
    {
      "epoch": 0.22204444154190273,
      "grad_norm": 4.908254146575928,
      "learning_rate": 1.7781115319315662e-05,
      "loss": 6.797,
      "step": 1700
    },
    {
      "epoch": 0.2233505853156786,
      "grad_norm": 4.118581771850586,
      "learning_rate": 1.776805537416743e-05,
      "loss": 6.7703,
      "step": 1710
    },
    {
      "epoch": 0.22465672908945453,
      "grad_norm": 4.879140853881836,
      "learning_rate": 1.77549954290192e-05,
      "loss": 6.7046,
      "step": 1720
    },
    {
      "epoch": 0.2259628728632304,
      "grad_norm": 4.839681148529053,
      "learning_rate": 1.774193548387097e-05,
      "loss": 6.7311,
      "step": 1730
    },
    {
      "epoch": 0.22726901663700633,
      "grad_norm": 5.165290832519531,
      "learning_rate": 1.772887553872274e-05,
      "loss": 6.7976,
      "step": 1740
    },
    {
      "epoch": 0.2285751604107822,
      "grad_norm": 3.8216214179992676,
      "learning_rate": 1.7715815593574508e-05,
      "loss": 6.7736,
      "step": 1750
    },
    {
      "epoch": 0.22988130418455813,
      "grad_norm": 5.017007827758789,
      "learning_rate": 1.770275564842628e-05,
      "loss": 6.806,
      "step": 1760
    },
    {
      "epoch": 0.231187447958334,
      "grad_norm": 4.198180198669434,
      "learning_rate": 1.7689695703278047e-05,
      "loss": 6.7983,
      "step": 1770
    },
    {
      "epoch": 0.2324935917321099,
      "grad_norm": 4.728580474853516,
      "learning_rate": 1.7676635758129818e-05,
      "loss": 6.8086,
      "step": 1780
    },
    {
      "epoch": 0.23379973550588581,
      "grad_norm": 4.490848541259766,
      "learning_rate": 1.7663575812981585e-05,
      "loss": 6.8332,
      "step": 1790
    },
    {
      "epoch": 0.2351058792796617,
      "grad_norm": 4.2080183029174805,
      "learning_rate": 1.7650515867833357e-05,
      "loss": 6.7929,
      "step": 1800
    },
    {
      "epoch": 0.23641202305343761,
      "grad_norm": 4.660709381103516,
      "learning_rate": 1.7637455922685128e-05,
      "loss": 6.8082,
      "step": 1810
    },
    {
      "epoch": 0.2377181668272135,
      "grad_norm": 4.547085762023926,
      "learning_rate": 1.7624395977536896e-05,
      "loss": 6.7734,
      "step": 1820
    },
    {
      "epoch": 0.23902431060098941,
      "grad_norm": 4.584306240081787,
      "learning_rate": 1.7611336032388667e-05,
      "loss": 6.7678,
      "step": 1830
    },
    {
      "epoch": 0.2403304543747653,
      "grad_norm": 5.097013473510742,
      "learning_rate": 1.7598276087240434e-05,
      "loss": 6.6616,
      "step": 1840
    },
    {
      "epoch": 0.2416365981485412,
      "grad_norm": 4.913064002990723,
      "learning_rate": 1.7585216142092206e-05,
      "loss": 6.744,
      "step": 1850
    },
    {
      "epoch": 0.2429427419223171,
      "grad_norm": 4.083826541900635,
      "learning_rate": 1.7572156196943973e-05,
      "loss": 6.7982,
      "step": 1860
    },
    {
      "epoch": 0.244248885696093,
      "grad_norm": 4.313727855682373,
      "learning_rate": 1.7559096251795744e-05,
      "loss": 6.8011,
      "step": 1870
    },
    {
      "epoch": 0.2455550294698689,
      "grad_norm": 4.017637252807617,
      "learning_rate": 1.7546036306647512e-05,
      "loss": 6.8099,
      "step": 1880
    },
    {
      "epoch": 0.2468611732436448,
      "grad_norm": 5.158207416534424,
      "learning_rate": 1.7532976361499283e-05,
      "loss": 6.8083,
      "step": 1890
    },
    {
      "epoch": 0.2481673170174207,
      "grad_norm": 4.557510852813721,
      "learning_rate": 1.751991641635105e-05,
      "loss": 6.823,
      "step": 1900
    },
    {
      "epoch": 0.2494734607911966,
      "grad_norm": 4.951744079589844,
      "learning_rate": 1.7506856471202822e-05,
      "loss": 6.7339,
      "step": 1910
    },
    {
      "epoch": 0.2507796045649725,
      "grad_norm": 4.992070198059082,
      "learning_rate": 1.749379652605459e-05,
      "loss": 6.7571,
      "step": 1920
    },
    {
      "epoch": 0.25208574833874836,
      "grad_norm": 5.463634014129639,
      "learning_rate": 1.748073658090636e-05,
      "loss": 6.7166,
      "step": 1930
    },
    {
      "epoch": 0.2533918921125243,
      "grad_norm": 4.391646385192871,
      "learning_rate": 1.7467676635758132e-05,
      "loss": 6.7415,
      "step": 1940
    },
    {
      "epoch": 0.2546980358863002,
      "grad_norm": 4.594816207885742,
      "learning_rate": 1.74546166906099e-05,
      "loss": 6.7468,
      "step": 1950
    },
    {
      "epoch": 0.2560041796600761,
      "grad_norm": 4.614181041717529,
      "learning_rate": 1.744155674546167e-05,
      "loss": 6.7795,
      "step": 1960
    },
    {
      "epoch": 0.25731032343385196,
      "grad_norm": 5.320127010345459,
      "learning_rate": 1.742849680031344e-05,
      "loss": 6.7574,
      "step": 1970
    },
    {
      "epoch": 0.2586164672076279,
      "grad_norm": 4.698493957519531,
      "learning_rate": 1.741543685516521e-05,
      "loss": 6.8299,
      "step": 1980
    },
    {
      "epoch": 0.2599226109814038,
      "grad_norm": 5.362995624542236,
      "learning_rate": 1.740237691001698e-05,
      "loss": 6.7341,
      "step": 1990
    },
    {
      "epoch": 0.2612287547551797,
      "grad_norm": 4.771575927734375,
      "learning_rate": 1.738931696486875e-05,
      "loss": 6.7593,
      "step": 2000
    },
    {
      "epoch": 0.26253489852895556,
      "grad_norm": 5.15439510345459,
      "learning_rate": 1.737625701972052e-05,
      "loss": 6.706,
      "step": 2010
    },
    {
      "epoch": 0.2638410423027315,
      "grad_norm": 4.435590744018555,
      "learning_rate": 1.7363197074572288e-05,
      "loss": 6.7765,
      "step": 2020
    },
    {
      "epoch": 0.2651471860765074,
      "grad_norm": 4.398651599884033,
      "learning_rate": 1.735013712942406e-05,
      "loss": 6.715,
      "step": 2030
    },
    {
      "epoch": 0.26645332985028325,
      "grad_norm": 4.48677921295166,
      "learning_rate": 1.7337077184275827e-05,
      "loss": 6.8062,
      "step": 2040
    },
    {
      "epoch": 0.26775947362405916,
      "grad_norm": 4.992433547973633,
      "learning_rate": 1.7324017239127598e-05,
      "loss": 6.7837,
      "step": 2050
    },
    {
      "epoch": 0.2690656173978351,
      "grad_norm": 5.032288551330566,
      "learning_rate": 1.7310957293979366e-05,
      "loss": 6.7197,
      "step": 2060
    },
    {
      "epoch": 0.270371761171611,
      "grad_norm": 5.72601318359375,
      "learning_rate": 1.7297897348831137e-05,
      "loss": 6.6804,
      "step": 2070
    },
    {
      "epoch": 0.27167790494538685,
      "grad_norm": 4.324533462524414,
      "learning_rate": 1.7284837403682905e-05,
      "loss": 6.6693,
      "step": 2080
    },
    {
      "epoch": 0.27298404871916276,
      "grad_norm": 4.676733016967773,
      "learning_rate": 1.7271777458534676e-05,
      "loss": 6.7401,
      "step": 2090
    },
    {
      "epoch": 0.2742901924929387,
      "grad_norm": 5.060011386871338,
      "learning_rate": 1.7258717513386443e-05,
      "loss": 6.8009,
      "step": 2100
    },
    {
      "epoch": 0.27559633626671454,
      "grad_norm": 4.329906463623047,
      "learning_rate": 1.7245657568238215e-05,
      "loss": 6.7499,
      "step": 2110
    },
    {
      "epoch": 0.27690248004049045,
      "grad_norm": 4.262704849243164,
      "learning_rate": 1.7232597623089982e-05,
      "loss": 6.6749,
      "step": 2120
    },
    {
      "epoch": 0.27820862381426636,
      "grad_norm": 4.026113510131836,
      "learning_rate": 1.7219537677941753e-05,
      "loss": 6.7626,
      "step": 2130
    },
    {
      "epoch": 0.2795147675880423,
      "grad_norm": 4.406238079071045,
      "learning_rate": 1.720647773279352e-05,
      "loss": 6.7672,
      "step": 2140
    },
    {
      "epoch": 0.28082091136181814,
      "grad_norm": 5.032039642333984,
      "learning_rate": 1.7193417787645292e-05,
      "loss": 6.8167,
      "step": 2150
    },
    {
      "epoch": 0.28212705513559405,
      "grad_norm": 5.242878437042236,
      "learning_rate": 1.7180357842497064e-05,
      "loss": 6.6064,
      "step": 2160
    },
    {
      "epoch": 0.28343319890936997,
      "grad_norm": 6.347835540771484,
      "learning_rate": 1.7167297897348835e-05,
      "loss": 6.7434,
      "step": 2170
    },
    {
      "epoch": 0.2847393426831458,
      "grad_norm": 5.268312454223633,
      "learning_rate": 1.7154237952200602e-05,
      "loss": 6.7823,
      "step": 2180
    },
    {
      "epoch": 0.28604548645692174,
      "grad_norm": 4.320001602172852,
      "learning_rate": 1.7141178007052374e-05,
      "loss": 6.7465,
      "step": 2190
    },
    {
      "epoch": 0.28735163023069765,
      "grad_norm": 3.8029375076293945,
      "learning_rate": 1.712811806190414e-05,
      "loss": 6.7893,
      "step": 2200
    },
    {
      "epoch": 0.28865777400447357,
      "grad_norm": 4.121620178222656,
      "learning_rate": 1.7115058116755912e-05,
      "loss": 6.7185,
      "step": 2210
    },
    {
      "epoch": 0.2899639177782494,
      "grad_norm": 4.215663433074951,
      "learning_rate": 1.710199817160768e-05,
      "loss": 6.6105,
      "step": 2220
    },
    {
      "epoch": 0.29127006155202534,
      "grad_norm": 4.810055255889893,
      "learning_rate": 1.708893822645945e-05,
      "loss": 6.7563,
      "step": 2230
    },
    {
      "epoch": 0.29257620532580125,
      "grad_norm": 5.03648042678833,
      "learning_rate": 1.707587828131122e-05,
      "loss": 6.7416,
      "step": 2240
    },
    {
      "epoch": 0.2938823490995771,
      "grad_norm": 4.033822059631348,
      "learning_rate": 1.706281833616299e-05,
      "loss": 6.7314,
      "step": 2250
    },
    {
      "epoch": 0.295188492873353,
      "grad_norm": 6.743277549743652,
      "learning_rate": 1.7049758391014758e-05,
      "loss": 6.6679,
      "step": 2260
    },
    {
      "epoch": 0.29649463664712894,
      "grad_norm": 5.4207258224487305,
      "learning_rate": 1.703669844586653e-05,
      "loss": 6.7949,
      "step": 2270
    },
    {
      "epoch": 0.29780078042090485,
      "grad_norm": 4.820331573486328,
      "learning_rate": 1.7023638500718297e-05,
      "loss": 6.6749,
      "step": 2280
    },
    {
      "epoch": 0.2991069241946807,
      "grad_norm": 4.387054920196533,
      "learning_rate": 1.7010578555570068e-05,
      "loss": 6.7044,
      "step": 2290
    },
    {
      "epoch": 0.3004130679684566,
      "grad_norm": 3.8258588314056396,
      "learning_rate": 1.6997518610421836e-05,
      "loss": 6.7253,
      "step": 2300
    },
    {
      "epoch": 0.30171921174223254,
      "grad_norm": 5.161051273345947,
      "learning_rate": 1.6984458665273607e-05,
      "loss": 6.851,
      "step": 2310
    },
    {
      "epoch": 0.3030253555160084,
      "grad_norm": 4.399811267852783,
      "learning_rate": 1.6971398720125375e-05,
      "loss": 6.7526,
      "step": 2320
    },
    {
      "epoch": 0.3043314992897843,
      "grad_norm": 4.1422119140625,
      "learning_rate": 1.6958338774977146e-05,
      "loss": 6.7617,
      "step": 2330
    },
    {
      "epoch": 0.3056376430635602,
      "grad_norm": 5.727328300476074,
      "learning_rate": 1.6945278829828914e-05,
      "loss": 6.7988,
      "step": 2340
    },
    {
      "epoch": 0.30694378683733614,
      "grad_norm": 4.104597091674805,
      "learning_rate": 1.6932218884680688e-05,
      "loss": 6.7496,
      "step": 2350
    },
    {
      "epoch": 0.308249930611112,
      "grad_norm": 4.883167743682861,
      "learning_rate": 1.6919158939532456e-05,
      "loss": 6.6318,
      "step": 2360
    },
    {
      "epoch": 0.3095560743848879,
      "grad_norm": 3.991724729537964,
      "learning_rate": 1.6906098994384227e-05,
      "loss": 6.743,
      "step": 2370
    },
    {
      "epoch": 0.31086221815866383,
      "grad_norm": 4.8218183517456055,
      "learning_rate": 1.6893039049235995e-05,
      "loss": 6.7046,
      "step": 2380
    },
    {
      "epoch": 0.3121683619324397,
      "grad_norm": 4.119555950164795,
      "learning_rate": 1.6879979104087766e-05,
      "loss": 6.7379,
      "step": 2390
    },
    {
      "epoch": 0.3134745057062156,
      "grad_norm": 4.232973575592041,
      "learning_rate": 1.6866919158939534e-05,
      "loss": 6.6975,
      "step": 2400
    },
    {
      "epoch": 0.3147806494799915,
      "grad_norm": 5.055915832519531,
      "learning_rate": 1.6853859213791305e-05,
      "loss": 6.7473,
      "step": 2410
    },
    {
      "epoch": 0.31608679325376743,
      "grad_norm": 4.309212684631348,
      "learning_rate": 1.6840799268643073e-05,
      "loss": 6.7413,
      "step": 2420
    },
    {
      "epoch": 0.3173929370275433,
      "grad_norm": 4.99688720703125,
      "learning_rate": 1.6827739323494844e-05,
      "loss": 6.7679,
      "step": 2430
    },
    {
      "epoch": 0.3186990808013192,
      "grad_norm": 5.414090156555176,
      "learning_rate": 1.681467937834661e-05,
      "loss": 6.7905,
      "step": 2440
    },
    {
      "epoch": 0.3200052245750951,
      "grad_norm": 4.0569024085998535,
      "learning_rate": 1.6801619433198383e-05,
      "loss": 6.6874,
      "step": 2450
    },
    {
      "epoch": 0.32131136834887103,
      "grad_norm": 4.685469150543213,
      "learning_rate": 1.678855948805015e-05,
      "loss": 6.8495,
      "step": 2460
    },
    {
      "epoch": 0.3226175121226469,
      "grad_norm": 4.725218296051025,
      "learning_rate": 1.677549954290192e-05,
      "loss": 6.7715,
      "step": 2470
    },
    {
      "epoch": 0.3239236558964228,
      "grad_norm": 4.221778869628906,
      "learning_rate": 1.676243959775369e-05,
      "loss": 6.7968,
      "step": 2480
    },
    {
      "epoch": 0.3252297996701987,
      "grad_norm": 4.51408576965332,
      "learning_rate": 1.674937965260546e-05,
      "loss": 6.7658,
      "step": 2490
    },
    {
      "epoch": 0.3265359434439746,
      "grad_norm": 4.410616397857666,
      "learning_rate": 1.6736319707457228e-05,
      "loss": 6.8034,
      "step": 2500
    }
  ],
  "logging_steps": 10,
  "max_steps": 15314,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.6239106203648e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
